{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='notebook_header'><b>CS 309 - Robot Learning</b></p>\n",
    "<p class='notebook_header'>Homework 1</p>\n",
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 1: Linear Algebra</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following matrix/vector functions using NumPy operations.\n",
    "\n",
    "If the function's operation isn't possible for matrix or vector inputs, return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply(a, b):\n",
    "    try:\n",
    "        return a.dot(b)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def divide(a, b):\n",
    "    return None\n",
    "\n",
    "def transpose(a):\n",
    "    return a.transpose()\n",
    "\n",
    "def two_norm(a):\n",
    "    return np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using your code from above, solve the following equations. If an operation isn't possible, put None or comment with \"Not Possible\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\begin{bmatrix} 2 \\\\ 3 \\\\ 9 \\end{bmatrix}, \\:\n",
    "v = \\begin{bmatrix} -2 \\\\ 1 \\\\ 8 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [9]\n",
    "])\n",
    "\n",
    "v = np.array([\n",
    "    [-2],\n",
    "    [1],\n",
    "    [8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u + v = \\begin{bmatrix} 0 \\\\ 4 \\\\ 17 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u - v = \\begin{bmatrix} 4 \\\\ 2 \\\\ 1 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v = \\text{Not Possible} $$  \n",
    "\n",
    "$$ u \\div v = \\text{Not Possible} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [ 4]\n",
      " [17]]\n",
      "[[4]\n",
      " [2]\n",
      " [1]]\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_plus_v = add(u, v)\n",
    "print(u_plus_v)\n",
    "\n",
    "u_minus_v = subtract(u, v)\n",
    "print(u - v)\n",
    "\n",
    "u_mult_v = multiply(u, v)\n",
    "print(u_mult_v)\n",
    "\n",
    "u_div_v = divide(u, v)\n",
    "print(u_div_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u^{\\;T} * v = \\; 71 $$  \n",
    "\n",
    "$$ u * v^{\\;T} = \\begin{bmatrix} -4 & 2 & 17 \\\\ -6 & 3 & 24 \\\\ -18 & 9 & 72 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u^{\\;T} * u = \\; 94 $$  \n",
    "\n",
    "$$ \\left \\| u \\right \\|_{2}^{2} = 9.695359714832659 $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71]]\n",
      "[[ -4   2  16]\n",
      " [ -6   3  24]\n",
      " [-18   9  72]]\n",
      "[[94]]\n",
      "9.695359714832659\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_transpose_v = multiply(transpose(u), v)\n",
    "print(u_transpose_v)\n",
    "\n",
    "u_v_transpose = multiply(u, transpose(v))\n",
    "print(u_v_transpose)\n",
    "\n",
    "u_transpose_u = multiply(transpose(u), u)\n",
    "print(u_transpose_u)\n",
    "\n",
    "two_norm_u = two_norm(u)\n",
    "print(two_norm_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix} 1 & 6 & 5\\\\ 0 & -4 & -1\\\\ 7 & 2 & 3 \\end{bmatrix}, \\: \n",
    "B = \\begin{bmatrix} 3 & 1 & 1\\\\ 4 & -1 & 7\\\\ 7 & 0 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A + B = \\begin{bmatrix} 4 & 7 & 6 \\\\ 4 & -5 & 6 \\\\ 14 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A - B = \\begin{bmatrix} -2 & 5 & 4 \\\\ -4 & -3 & -8 \\\\0 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A * B = \\begin{bmatrix} 62 & -5 & 43 \\\\ -23 & 4 & -28 \\\\ 50 & 5 & 21 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A \\div B = \\text{Not Possible} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7  6]\n",
      " [ 4 -5  6]\n",
      " [14  2  3]]\n",
      "[[-2  5  4]\n",
      " [-4 -3 -8]\n",
      " [ 0  2  3]]\n",
      "[[ 62  -5  43]\n",
      " [-23   4 -28]\n",
      " [ 50   5  21]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "a = np.array([\n",
    "    [1, 6, 5],\n",
    "    [0, -4, -1],\n",
    "    [7, 2, 3]\n",
    "])\n",
    "b = np.array([\n",
    "    [3, 1, 1],\n",
    "    [4, -1, 7],\n",
    "    [7, 0, 0]\n",
    "])\n",
    "a_plus_b = add(a, b)\n",
    "print(a_plus_b)\n",
    "\n",
    "a_minus_b = subtract(a, b)\n",
    "print(a_minus_b)\n",
    "\n",
    "a_mult_b = multiply(a, b)\n",
    "print(a_mult_b)\n",
    "\n",
    "a_div_b = divide(a, b)\n",
    "print(a_div_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C = \\begin{bmatrix} 5 & 1 \\\\ -1 & 7 \\\\ 3 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[5, 1],\n",
    "              [-1, 7],\n",
    "              [3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right Pseudo Inverse of C:\n",
    "$$\n",
    "\\begin{bmatrix} -0.25 & 0 & 0.5 \\\\ 0.03125 & 0.1484375 & 2.5625 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "Left Pseudo Inverse of C:\n",
    "\\begin{bmatrix} 0.1443299 & -0.02061856 & 0.08591065 \\\\ 0.0257732 & 0.13917526 & 0.00343643 \\end{bmatrix}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25       0.         0.5      ]\n",
      " [ 0.03125    0.1484375  0.0625   ]]\n",
      "[[ 0.1443299  -0.02061856  0.08591065]\n",
      " [ 0.0257732   0.13917526  0.00343643]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "right_pinv = c.transpose().dot(np.linalg.inv(c.dot(c.transpose())))\n",
    "print(right_pinv)\n",
    "\n",
    "left_pinv = (np.linalg.inv(c.transpose().dot(c)).dot(c.transpose()))\n",
    "print(left_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 2: Regression</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ordinary Least Squares below. \n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "$$ \\sum_{i = 1}^n (y_i - \\sum_{a = 1}^m b_ax_a + b_0)^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** Ordinary Least Squares in terms of what it optimizes.\n",
    "\n",
    "**Ordinary Least Squares minimizes the sum of the squared residuals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change this cell!\n",
    "# Load in the data about the study on students\n",
    "train = np.loadtxt('train.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = train.T\n",
    "X_train = np.array([x_0, x_1, x_2]).T\n",
    "Y_train = np.expand_dims(y, 1)\n",
    "\n",
    "test = np.loadtxt('test.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = test.T\n",
    "X_test = np.array([x_0, x_1, x_2]).T\n",
    "Y_test = np.expand_dims(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an imaginary study done on 101 students at Crest University. The study surveyed students for the amount they have spent on electronics, books, pencils, and foods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the **amount students spend on electronics ($Y$)** is linearly related to the **amount they spend on books ($ X_{0} $), pencils ($ X_{1}$)**, and **food ($ X_{2}$)**, \n",
    "**implement** the Ordinary Least Squares method to model this regression problem.\n",
    "\n",
    "The data is read in from the previous cell code. **X_train** has the input features, while **Y_train** has corresponding target outputs.\n",
    "\n",
    "After finding a solution, try to measure the error between your predictions and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 22.791845424466484\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for OLS here. DO NOT use any other libraries in your first implementation.\n",
    "\n",
    "def OLS(X, y):\n",
    "    return np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = OLS(X_train,Y_train)\n",
    "\n",
    "# TODO: test your model on testing data\n",
    "predicted_y = np.dot(X_train, theta)\n",
    "error = ((predicted_y - Y_train) ** 2).mean()\n",
    "print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what collinearity is.\n",
    "\n",
    "**Collinearity is a condition where variables are highly correlated such that they express a linear relationship in a regression model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ridge Regression below. \n",
    "\n",
    "$$ \\theta = (X^TX + \\lambda I)^{-1}X^TY $$\n",
    "\n",
    "$$ min(\\sum_{i = 1}^n (y_i - \\sum_{a = 1}^m b_ax_a + b_0)^2 + \\lambda \\sum_{a = 1}^m b_a^2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what the purpose ridge regression and its advantages and disadvantages over OLS.\n",
    "\n",
    "- RRS increases bias, but reduces model variance\n",
    "\n",
    "**Ridge Regression is a technique to use when the data has collinearity. If OLS were used in these circumstances, the large variances between estimates would deviate the observed value away from the actual value. However, when you have more features than observations, Ridge Regression can create bias.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** your regression model with ridge regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.92516587]\n",
      " [-0.70151584]\n",
      " [ 0.54358107]]\n",
      "Error: 22.785151592876804\n",
      "[[ 0.35554326]\n",
      " [-0.04593887]\n",
      " [ 0.48139218]]\n",
      "2Error: 3113.397458772699\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Ridge Regression here. DO NOT use any other libraries\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "def RR(X, y, ridge=0.001):\n",
    "    return np.linalg.solve(np.dot(X.T, X) + ridge * np.identity(len(X.T)), np.dot(X.T, y))\n",
    "#     return (np.linalg.inv((X.T).dot(X) + ridge * np.identity(len(X.T)))).dot((X.T).dot(y))\n",
    "#     pass\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = RR(X_train,Y_train)\n",
    "print(theta)\n",
    "\n",
    "predicted_y = np.dot(X_train, theta)\n",
    "error = ((predicted_y - Y_train) ** 2).mean()\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "# Testing various values for the ridge parameter\n",
    "theta = RR(X_train,Y_train, 500000)\n",
    "print(theta)\n",
    "\n",
    "predicted_y = np.dot(X_train, theta)\n",
    "error = ((predicted_y - Y_train) ** 2).mean()\n",
    "print(f\"2Error: {error}\")\n",
    "\n",
    "\n",
    "\n",
    "# TODO: test your model on testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences ridge regression created for theta compared to OLS, and why these differences even existed. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "**Our theta values affected by ridge regression were slightly smaller compared to those affected by OLS. This is because we have a shrinkage penalty within ridge regression, for the purpose of decreasing the variance of the line, that doesn’t exist within OLS. Larger values for the ridge parameter increase the error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other regularizers other than ridge regression, such as LASSO. **Explain** the differences between LASSO and Ridge Regression and how it changes the solution mathematically.\n",
    "\n",
    "The shrinkage penalty for LASSO is $$\\lambda \\sum_{a=1}^m |b_a|$$ whereas the shrinkage penalty for Ridge Regression is $$\\lambda \\sum_{a=1}^m b_a^2$$\n",
    "\n",
    "**With the shrinkage penaly for LASSO, we get a more robust solution, but it is more unstable than Ridge Regression. We also have the potential for multiple solutions whereas with Ridge Regression, we always have one solution. LASSO also has the potential for feature selection whereas Ridge Regression does not. Ridge Regression has analytical solutions whereas LASSO does not, and LASSO can have sparse outputs whereas Ridge Regression does not.**\n",
    "\n",
    "**LASSO tends to zero out parameters whereas Ridge Regression does not.**\n",
    "\n",
    "**Additionally, LASSO does both parameter shrinkage and variable selection automatically. However, LASSO does not work well with collinearity, because it might randomly choose one the collinear variables, possibly eliminating relevant independent variables. Conversely, Ridge Regression does not eliminate coefficient even if the variables are irrelevant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.99099792  2.01763613 -0.00368638]\n",
      "Error: 74685.8140478333\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for LASSO here\n",
    "# TODO: Plot your regression line over the input points. \n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "def LASSO(X, y, ridge=.001):\n",
    "    clf = linear_model.Lasso(alpha=ridge)\n",
    "    clf.fit(X, y)\n",
    "    return clf.coef_\n",
    "  \n",
    "\n",
    "# TODO: check against training data\n",
    "theta = LASSO(X_train,Y_train)\n",
    "print(theta)\n",
    "\n",
    "predicted_y = np.dot(X_train, theta)\n",
    "error = ((predicted_y - Y_train) ** 2).mean()\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "\n",
    "# TODO: check against testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the effect elastic nets had on your values for theta compared to OLS. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "\n",
    "\n",
    "**Our theta values affected by elastic nets were larger compared to those affected by OLS. Larger values for the ridge parameter increase the error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences between LASSO, Ridge Regression and Elastic Nets and how it changes the solution mathematically.\n",
    "\n",
    "**Elastic Net combines feature elimination from LASSO and feature coefficient reduction from Ridge Regression, reducing the impact of different features while not eliminating all of them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 74679.33668781767\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Elastic Nets here. You can use a library such as scipy\n",
    "# TODO: Plot your regression line over the input points.\n",
    "# from scipy\n",
    "from sklearn.linear_model import ElasticNet\n",
    "def EN(X,y):\n",
    "    regr = ElasticNet(random_state=0)\n",
    "    regr.tol = .001\n",
    "    regr.fit(X, y)\n",
    "    return regr.coef_\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = EN(X_train,Y_train)\n",
    "predicted_y = np.dot(X_train, theta)\n",
    "error = ((predicted_y - Y_train) ** 2).mean()\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "# TODO: check against testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of a regularizer?\n",
    "\n",
    "**A regularizer can prevent overfitting and complications to do with collinearity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give two examples where a regularizer would give more robust models.\n",
    "\n",
    "**A regularizer would give more robust models where... 1) your data has large outliers, and 2) your data sets are small.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain with reference to the dataset why a regularizer achieved better performance than OLS.\n",
    "\n",
    "**Our dataset had collinearity, reducing the precision of the estimate coefficients in OLS. The use of a regularizer minimized this issue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature transformation to fit a line to the curve generated from the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('feature_transform.csv', delimiter=',')\n",
    "y = X[:,2].reshape(500, 1)\n",
    "X = X[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bd75edbb5eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoly_coefficents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# TODO: Write the lambda function phi which will transform X\n",
    "# TODO: Plot the transformation and the resulting line after transforming\n",
    "\n",
    "def phi(x, degree):\n",
    "    # construct X matrix\n",
    "    columns = []\n",
    "    n = len(x)\n",
    "    for i in range(degree + 1):\n",
    "        if i == 0:\n",
    "            columns.append(np.ones(n))\n",
    "        else:\n",
    "            columns.append(x**i)\n",
    "    X = np.matrix(columns).T \n",
    "    return X\n",
    "\n",
    "X_tran = phi(X[:,0], 3)\n",
    "theta = OLS(X_tran, y)\n",
    "y_hat = np.dot(X_tran, theta)\n",
    "coef = np.array(theta).ravel()\n",
    "phi_plot = np.linspace(0, 1, 500)\n",
    "\n",
    "def poly_coefficents(x, coeffs):\n",
    "    # Returns a polynomial for 'x' values for the 'coeffs' provided.\n",
    "    k = len(coeffs)\n",
    "    y = 0\n",
    "    for i in range(k):\n",
    "        y += coeffs[i]*x**i\n",
    "    return y\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(phi_plot, poly_coefficents(phi_plot, coef))\n",
    "plt.scatter(X[:,0],y,color=(0.0,0.5,0.0))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what you learned about polynomial regression and explain what is happening to the model as you increase the degrees. Run the cell below and use the slider to help you.\n",
    "\n",
    "**As the slider increases, the degree of the polynomial used to represent the data increases with it. Because the line of best fit’s equation has changed, the model changes as well. This can be problematic because the polynomial begins to fit the training data really well but can start to lose meaning for any other data we’d like to test our model on. This is seen in the example because once the degree of the polynomial extends past 7 the end of the line begins to no longer fit the data well. It either abruptly goes up or down.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cb8cb92395a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT ALTER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# DO NOT ALTER\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "data = pd.concat([data,pd.Series(boston.target,name='MEDV')],axis=1)\n",
    "\n",
    "X = data[['LSTAT']].values\n",
    "y = data['MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "temp = pd.DataFrame({'x':x_train.reshape(1, 354)[0], 'y':y_train})\n",
    "temp = temp.sort_values('x')\n",
    "x_train = temp['x'].values.reshape(354,1)\n",
    "y_train = temp['y'].values\n",
    "\n",
    "temp = pd.DataFrame({'x':x_test.reshape(1, 152)[0], 'y':y_test})\n",
    "temp = temp.sort_values('x')\n",
    "x_test = temp['x'].values.reshape(152,1)\n",
    "y_test = temp['y'].values\n",
    "\n",
    "def f(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(x_train,y_train)\n",
    "    y_plot = model.predict(x_test)\n",
    "    \n",
    "    plt.scatter(x_train, y_train, s=10, color='red', alpha=.3)\n",
    "    plt.scatter(x_test, y_test, s=10)\n",
    "\n",
    "    test_sr = (y_test - y_plot)**2\n",
    "    test_ssr = test_sr.sum()\n",
    "    test_asr = test_ssr/len(test_sr)\n",
    "    \n",
    "    y_plot_train = model.predict(x_train)\n",
    "    train_sr = (y_train - y_plot_train)**2\n",
    "    train_ssr = train_sr.sum()\n",
    "    train_asr = train_ssr/len(train_sr)\n",
    "    \n",
    "    plt.plot(x_test, y_plot, label=\"degree %d\" % degree + '; Test Error: %.2f' % test_asr + '; Train Error: %.2f' % train_asr, color='green')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "interact(f, degree = widgets.IntSlider(min=1, max=20, step=1, value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed by Jennifer Mickel and Rebecca Riley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
